{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMDB Dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "print(os.listdir(\"./input\"))\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLength = 100\n",
    "maxVocabNumber = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text, removeStopwords=True, performStemming=True):\n",
    "    \n",
    "    remove_special_char = re.compile('r[^a-z\\d]', re.IGNORECASE)\n",
    "    replace_numerics = re.compile(r'\\d+', re.IGNORECASE)\n",
    "    text = remove_special_char.sub('', text)\n",
    "    text = replace_numerics.sub('', text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    text = text.lower().split()\n",
    "\n",
    "    \n",
    "    processedText = list()\n",
    "    for word in text:        \n",
    "        if removeStopwords:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "        if performStemming:\n",
    "            word = stemmer.stem(word)\n",
    "            \n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        word = lemmatizer.lemmatize(word, 'v')\n",
    "            \n",
    "        processedText.append(word)\n",
    "\n",
    "    text = ' '.join(processedText)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('./input/IMDB Dataset.csv')\n",
    "X = [cleanText(text) for text in list(imdb['review'])]\n",
    "Y = [1 if sentiment=='positive' else 0 for sentiment in list(imdb['sentiment'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=maxVocabNumber)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized = pad_sequences(tokenizer.texts_to_sequences(X), maxlen=maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          10000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 120)          77280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 10,084,601\n",
      "Trainable params: 10,084,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxLength,))\n",
    "embedding = Embedding(maxVocabNumber, 100)(inp)\n",
    "bilstm = Bidirectional(LSTM(60, return_sequences = True))(embedding)\n",
    "maxpool = GlobalMaxPool1D()(bilstm)\n",
    "out = Dense(60, activation='relu')(maxpool)\n",
    "out = Dense(1, activation='sigmoid')(out)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 173s 7ms/step - loss: 0.6810 - acc: 0.6376 - val_loss: 0.6195 - val_acc: 0.8033\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 176s 7ms/step - loss: 0.3940 - acc: 0.8436 - val_loss: 0.3382 - val_acc: 0.8678\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.1708 - acc: 0.9400 - val_loss: 0.3352 - val_acc: 0.8712\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 192s 8ms/step - loss: 0.0774 - acc: 0.9775 - val_loss: 0.4203 - val_acc: 0.8666\n"
     ]
    }
   ],
   "source": [
    "batchSize = 1000\n",
    "epochs = 4\n",
    "hist = model.fit(X_tokenized, Y, batch_size=batchSize, epochs=epochs, verbose=1, shuffle=True, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0   21\n",
      "     3 1552]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.18235047]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = [\"It is a very bad movie and very disgusting\"]\n",
    "review = [cleanText(text) for text in review]\n",
    "review = pad_sequences(tokenizer.texts_to_sequences(review),maxlen=maxLength)\n",
    "print(review)\n",
    "model.predict(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"sentimentanalysis.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "model = load_model('sentimentanalysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97607625]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
